# -*- coding: utf-8 -*-
"""EDA-EstadisticasJugadoresFIFA2021.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jsF60n8Cn-0_Q2Fa2HUm3ZSyRrccncnf

# Dataset: Estadísticas de jugadores de Fifa 2021.

### Objetivo GENERAL:
* Desarrollar un modelo de regresión lineal que prediga el Overall (OVA) de los jugadores de FIFA 21 basado en sus características, con el fin de entender y predecir la calidad general de los jugadores en el juego.

### Objetivos ESPECIFICOS:
1. Preprocesamiento de Datos:
  * Realizar un análisis exploratorio de datos (EDA) para comprender la distribución y la calidad de los datos, así como identificar posibles problemas como valores faltantes y valores atípicos.
  * Codificar las características categóricas y manejar los valores faltantes de manera apropiada para preparar los datos para el modelado.
2. Modelado de Regresión Lineal:
  * Construir un modelo de regresión lineal utilizando las características seleccionadas como variables predictoras y el OVA como la variable objetivo.
  * Evaluar el rendimiento del modelo utilizando métricas de evaluación de regresión, como el error cuadrático medio (MSE) y el coeficiente de determinación (R²).
3. Selección de Características:
  * Realizar análisis de importancia de características para identificar las características más relevantes para predecir el OVA de los jugadores.
  * Explorar técnicas de selección de características, como la eliminación recursiva de características (RFE), para mejorar la precisión del modelo.
4. Validación del Modelo:
  * Dividir el conjunto de datos en conjuntos de entrenamiento y prueba para evaluar el rendimiento del modelo en datos no vistos.
  * Utilizar técnicas de validación cruzada para garantizar la robustez del modelo y evitar el sobreajuste.
5. Interpretación de Resultados:
  * Interpretar los coeficientes del modelo de regresión lineal para comprender la relación entre las características de los jugadores y su OVA.
  * Identificar las características que más influyen en la calidad general de los jugadores según el modelo.

### Integrantes:
- Natalia Mejía.
- Miguel Uribe.
- María Andrea Méndez.
- Nicolás Reyes.

# 1. EDA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import six


import jinja2
from tabulate import tabulate

"""# 1.1 Análisis preliminar del data set
Hemos quitado algunas columnas, dado que al consultar con expertos de FIFA 21, nos dieron información de aquellas columnas que solo hacen parte del data set para fines complementarios que no representan un valor significativo a nuestro análisis. Adicionalmente, hay columnas que se encuentran completamente vacías o que continen el mismo dato para todas las filas.
"""

#Leyendo dataset
df = pd.read_csv('/content/fifa21_male2.csv', low_memory=False)
df = df.sample(n=1000)

#Drop de columnas con valores mezclados
df = df.drop(["Hits","Value", "Wage", "Release Clause", "ID", "Name", "Club", "Position", "Player Photo", "Club Logo", "Flag Photo", "Team & Contract", "Growth", "Joined", "Loan Date End", "Contract", "Gender", "Composure", "W/F", "SM", "A/W", "D/W", "IR", "LS", "ST", "RS", "LW", "LF", "CF", "RF", "RW", "LAM", "CAM", "RAM", "LM", "LCM", "CM", "RCM", "RM", "LWB", "LDM", "CDM", "RDM", "RWB", "LB", "LCB", "CB", "RCB", "RB", "GK", "BP"], axis = 1)
#Codificación de foot
df

"""# 1.2 Limpieza de Datos.
Despues de quitar unas columnas de manera preliminar, decidimos modificar las columnas de "Height" y "Weight" a métricas estadarizadas internacionalmente sin unidades de medida. Por otro lado, se deben quitar las filas que contengan muchos nulos ya que pueden afectar negativamente la predicción.
"""

def convertir_a_cm(altura):
    pies, pulgadas = altura.split("'")
    pies = int(pies)
    pulgadas = int(pulgadas[:-1])  # Elimina el último carácter (la comilla doble)
    altura_cm = (pies * 12 + pulgadas) * 2.54
    return altura_cm
def convertir_a_kg(peso):
    peso_lbs = int(peso[:-3])  # Elimina "lbs" y convierte a entero
    peso_kg = peso_lbs * 0.453592  # 1 lb = 0.453592 kg
    return peso_kg

df['Weight'] = df['Weight'].apply(convertir_a_kg)
df.Weight

df['Height'] = df['Height'].apply(convertir_a_cm)
df.Height

df.isnull().sum()

df = df.dropna()
df

df.isnull().sum()

"""Al notar que habían filas que contenían columnas en nulo, se eliminaron.

# 1.3. Gráficas del EDA.
"""

df

df.shape

df.isnull().sum()

columns=df.columns
columns

sns.countplot(x = 'OVA', data = df)

"""### Conclusiones

Se puede observar que el dataset tiene un total de 995 filas y 58 columnas. De estas, ninguna fila o registro tiene valores nulos. Adicionalmente, se puede ver que el OVA (nuestro objetivo) se encuentra distribuido de forma normal y no se encuentran valores fuera del rango esperado.
"""

# Calcular la varianza de cada columna (excluyendo las columnas "Nationality" y "foot")
variances = df.drop(columns=df.select_dtypes(exclude=[np.number]).columns).var()

# Calcular la media de cada columna (excluyendo las columnas "Nationality" y "foot")
means = df.drop(columns=df.select_dtypes(exclude=[np.number]).columns).mean()

# Calcular la moda de cada columna (excluyendo las columnas "Nationality" y "foot")
modes = df.drop(columns=df.select_dtypes(exclude=[np.number]).columns).mode().iloc[0]  # Seleccionar solo el primer valor de la serie de moda

# Calcular la varianza, media y moda de la columna "Nationality" por separado
nationality_mode = df["Nationality"].mode().iloc[0]  # Seleccionar solo el primer valor de la serie de moda

# Calcular la varianza, media y moda de la columna "foot" por separado

foot_mode = df["foot"].mode().iloc[0]  # Seleccionar solo el primer valor de la serie de moda

# Imprimir los resultados
data = []
columnas_numericas = df.select_dtypes(include=[np.number])
for column in columnas_numericas:
      data.append([column, variances[column], means[column], modes[column]])

data.append(["Nationality", '','' ,nationality_mode])
data.append(["foot", '','' ,foot_mode])

# Mostrar la tabla con tabulate
table = tabulate(data, headers=["Columna", "Varianza", "Media", "Moda"], tablefmt="grid")
print(table)
num_columnas = columnas_numericas.shape[1]
print("Número de filas en columnas_no_numericas:", num_columnas)

for column in df.columns:
    # Crear una figura con dos subgráficos
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Histograma
    sns.histplot(df[column], ax=axes[0], kde=True, color='skyblue', edgecolor='black')
    axes[0].set_title('Histograma de {}'.format(column))
    axes[0].set_xlabel(column)
    axes[0].set_ylabel('Frecuencia')

    # Diagrama de caja y bigotes
    sns.boxplot(x=df[column], ax=axes[1], color='lightgreen', linewidth=1)
    axes[1].set_title('Diagrama de caja de {}'.format(column))
    axes[1].set_xlabel(column)

    # Ajustar el diseño
    plt.tight_layout()

    # Mostrar los gráficos
    plt.show()

# Eliminar las columnas "Nationality" y "foot" del DataFrame
df_filtered = df.drop(columns=df.select_dtypes(exclude=[np.number]).columns)

# Calcular la matriz de correlación
corr_matrix = df_filtered.corr()

# Aplicar un estilo de gradiente de color a la matriz de correlación
corr_matrix_styled = corr_matrix.style.background_gradient(cmap=plt.cm.coolwarm)

# Mostrar la matriz de correlación con el estilo aplicado
corr_matrix_styled

columnas_a_mantener = ["Age", "Nationality", "foot", "POT", "OVA", "BOV", "Height", "PAC", "Base Stats", "Total Stats", "SHO", "PAS", "DRI", "DEF", "PHY"]

# Encuentra las columnas que deseas eliminar
columnas_a_eliminar = [col for col in df.columns if col not in columnas_a_mantener]

# Dropea las columnas que deseas eliminar
df = df.drop(columns=columnas_a_eliminar)

"""### Conclusion
Con base en este diagrama, se puede ver la correlación que tiene las diferentes variables. Teniendo en cuenta que en un principio se buscaba predecir el OVA, se mira específicamente esta columna. Las correlaciones en esas columnas no son fuertes, por lo tanto en un futuro puede que el modelo presente errores y sea necesario cambiar el objetivo.

#Árbol de decisión

##1.1 Normalización de los datos categóricos
"""

#Codificación de frecuencia para "Nationality" cuyos valores son nombres de países
# Calcular la frecuencia de cada categoría
frecuencia = df['Nationality'].value_counts()

# Aplicar la codificación de frecuencia
df['nationality_frecuencia'] = df['Nationality'].map(frecuencia)


#Codificación one hot para "foot" que contiene valores de left y right
df = pd.get_dummies(df, columns=['foot'])

# Imprimir el dataframe codificado

print(df)

df = df.drop(["Nationality"], axis=1)
df

#División del dataset

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

X = df[df.columns] # Features
X = df.drop(columns=['OVA'])

y = df.OVA # Target variable
nombres_columnas = X.columns.tolist()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

X_train.to_csv('train.csv', index=False)
X_test.to_csv('test.csv', index=False)

X_train2, X_test2, y_train2, y_test_2 = train_test_split(X_train, y_train, test_size=0.2, random_state=1)

"""##1.2 Modelo del árbol de decisión"""

#Modelo del árbol

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn.model_selection import train_test_split # Import train_test_split function
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation

# feature_cols = ['Age', 'BOV','POT','Height','Weight', 'Base Stats', 'PAC', 'SHO', 'PAS','DRI', 'DEF', 'PHY']

# Create Decision Tree classifer object
clf = DecisionTreeClassifier(max_depth=9, max_features=20)

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

!pip install graphviz

!pip install pydotplus

#Visualización de árbol de decisión
from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = nombres_columnas)
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('fifa.png')
Image(graph.create_png())

"""## Conclusiones del Árbol
Ya que el arbol esta contruido con 17 columnas, es bastante extenso y dificil de interpretar. Sin embargo, decidimos continuar con las columnas que originalmente se tenían planeadas usar (Las que los expertos nos recomendaron) ya que segun el analisis, nos dan un accuracy aceptable.

# Conclusiones Generales

A partir de el EDA y la limpieza de datos, hemos definido las columnas con las cuales se va a armar el modelo, y los conjuntos de datos (datasets) que se van a usar para entrenar, probar y validar el algoritmo.

# K-means
"""

from sklearn.cluster import KMeans

df.PAC

X = df

#Codo
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')

"""Como se puede ver en el gráfico del codo, el punto de inflexión que aparece es en el 3, lo cual indica que el número adecuado de clusters para este caso es de 3."""

kmeans = KMeans(n_clusters = 4, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)

y_kmeans

df.drop("Clase", axis=1)

nueva_columna = [0, 0, 0, 1, 0, 3, 2, 0, 1, 3, 1, 3, 1, 3, 0, 2, 3, 1, 1, 1, 1, 3, 3, 0, 2, 3, 0, 3, 3, 3, 3, 0, 1, 3, 1, 3, 0, 0, 3, 3, 0, 3, 3, 2, 0, 0, 3, 1, 3, 1, 1, 3, 3, 0, 1, 0, 3, 3, 0, 3, 3, 0, 1, 3, 3, 3, 0, 1, 3, 1, 3, 3, 3, 0, 0, 0, 0, 3, 1, 3, 0, 3, 2, 3, 3, 1, 0, 0, 0, 0, 1, 3, 2, 0, 0, 1, 3, 3, 3, 3, 1, 1, 0, 0, 1, 0, 1, 1, 3, 2, 2, 0, 1, 0, 2, 3, 1, 0, 1, 0, 0, 3, 3, 0, 0, 1, 1, 0, 3, 1, 1, 3, 1, 3, 0, 3, 0, 1, 3, 3, 3, 0, 0, 3, 2, 3, 0, 3, 0, 0, 0, 0, 1, 3, 1, 1, 2, 3, 0, 0, 3, 3, 3, 0, 3, 0, 0, 3, 0, 1, 0, 3, 2, 1, 1, 1, 2, 0, 0, 0, 2, 1, 2, 0, 3, 0, 2, 0, 0, 1, 3, 3, 0, 3, 0, 3, 3, 0, 0, 3, 1, 2, 0, 3, 1, 1, 1, 3, 1, 0, 0, 1, 0, 3, 1, 0, 3, 3, 3, 1, 3, 2, 0, 1, 3, 1, 1, 1, 2, 1, 3, 1, 3, 2, 1, 2, 3, 0, 3, 1, 3, 3, 3, 0, 0, 0, 1, 0, 2, 3, 1, 1, 0, 1, 1, 1, 2, 1, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 1, 3, 0, 3, 3, 2, 3, 1, 1, 1, 3, 0, 0, 2, 3, 0, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 0, 1, 1, 1, 3, 0, 0, 3, 0, 2, 2, 3, 1, 0, 1, 1, 2, 1, 3, 1, 3, 3, 1, 0, 0, 1, 1, 0, 0, 0, 3, 2, 3, 3, 1, 1, 3, 1, 2, 2, 1, 3, 0, 3, 3, 2, 3, 2, 3, 1, 2, 2, 0, 0, 0, 3, 1, 0, 0, 1, 3, 1, 3, 1, 3, 2, 3, 1, 1, 3, 3, 0, 0, 3, 1, 1, 1, 0, 1, 3, 3, 1, 1, 3, 0, 0, 0, 1, 0, 0, 3, 3, 0, 0, 0, 0, 1, 1, 0, 0, 3, 0, 3, 3, 0, 1, 1, 3, 0, 1, 0, 2, 2, 1, 3, 0, 3, 3, 0, 3, 1, 3, 1, 1, 3, 0, 3, 3, 0, 3, 3, 1, 1, 3, 0, 1, 1, 3, 2, 2, 0, 0, 0, 0, 0, 2, 3, 0, 1, 3, 1, 3, 2, 3, 2, 0, 1, 3, 0, 0, 1, 3, 2, 3, 3, 1, 3, 3, 1, 0, 0, 1, 3, 2, 3, 0, 3, 3, 0, 1, 3, 0, 1, 1, 1, 0, 2, 0, 0, 2, 0, 0, 1, 1, 1, 2, 3, 3, 0, 3, 0, 3, 3, 0, 3, 2, 2, 3, 1, 3, 2, 0, 1, 0, 3, 1, 2, 2, 2, 3, 3, 1, 2, 0, 2, 3, 1, 3, 3, 0, 1, 1, 1, 1, 0, 1, 3, 0, 2, 3, 1, 1, 1, 3, 0, 2, 3, 1, 2, 3, 3, 3, 1, 2, 1, 3, 0, 0, 2, 3, 0, 1, 2, 1, 3, 0, 0, 0, 1, 0, 1, 3, 0, 3, 3, 1, 2, 1, 0, 3, 3, 1, 3, 2, 3, 1, 2, 3, 1, 1, 1, 3, 0, 1, 0, 3, 3, 0, 3, 1, 3, 3, 2, 0, 3, 3, 1, 0, 3, 3, 1, 1, 2, 1, 0, 0, 0, 3, 1, 3, 0, 3, 3, 0, 2, 1, 1, 2, 3, 2, 2, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 3, 3, 3, 3, 2, 1, 0, 0, 0, 1, 0, 3, 1, 1, 0, 1, 1, 0, 0, 1, 3, 1, 3, 3, 2, 2, 0, 0, 0, 1, 2, 3, 0, 3, 1, 3, 3, 3, 2, 3, 1, 1, 2, 3, 3, 3, 0, 0, 3, 3, 1, 3, 3, 0, 3, 3, 3, 3, 1, 1, 1, 2, 2, 0, 2, 0, 3, 3, 0, 3, 2, 0, 3, 2, 3, 3, 3, 3, 0, 2, 2, 1, 1, 3, 1, 3, 0, 1, 3, 3, 1, 2, 0, 1, 3, 2, 1, 3, 2, 1, 2, 3, 1, 0, 3, 0, 0, 1, 1, 0, 3, 3, 0, 1, 0, 1, 3, 0, 2, 3, 1, 3, 3, 1, 1, 0, 0, 3, 3, 2, 0, 1, 1, 2, 3, 3, 1, 2, 3, 0, 1, 0, 2, 1, 3, 3, 1, 2, 1, 0, 0, 3, 2, 3, 1, 3, 2, 2, 3, 2, 0, 0, 2, 3, 0, 2, 1, 1, 3, 1, 3, 1, 1, 3, 0, 0, 0, 1, 2, 2, 1, 2, 1, 0, 1, 1, 0, 0, 3, 3, 0, 0, 1, 2, 0, 1, 3, 1, 3, 3, 1, 2, 0, 0, 0, 3, 0, 3, 1, 3, 0, 0, 0, 1, 1, 1, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 2, 1, 1, 3, 3, 2, 0, 1, 3, 0, 3, 0, 1, 1, 1, 0, 1, 1, 3, 0, 1, 3, 0, 0, 1, 3, 3, 3, 0, 3, 3, 3, 1, 0, 0, 0, 3, 1, 3, 2, 3, 1, 0, 0, 2, 0, 2, 3, 0, 3, 0, 1, 0, 2, 1, 3, 3, 0, 3, 3, 3, 0, 3, 3, 1, 3, 2, 0, 1, 0, 3, 0, 3, 0, 1, 0, 1, 0, 0, 3, 1, 0, 0, 3, 0, 3, 1, 0, 1, 2, 1, 3, 3, 3, 3, 3, 3, 3, 1, 2, 0, 0, 3, 3, 1, 1, 3, 1, 3, 3, 1, 0, 1, 3, 3, 1, 0, 3, 0, 1, 3]

# Agregar una nueva columna al DataFrame con los valores del arreglo
df['Clase'] = nueva_columna

df

# Para facilitar la estadistica descriptiva decidimos ordenar el data set mediante la numeración de la nueva columna (de 0 a 3).

df_ordenado = df.sort_values(by='Clase')

df_ordenado

# Decidimos separar el dataset en 4 grandes grupos e ir mirando ciertas caracteristicas para saber si son las relevantes en su clasificación.

subset1 = df[df['Clase'] == 0]
subset2 = df[df['Clase'] == 1]
subset3 = df[df['Clase'] == 2]
subset4 = df[df['Clase'] == 3]

descripcion1 = subset1.describe()
descripcion2 = subset2.describe()
descripcion3 = subset3.describe()
descripcion4 = subset4.describe()

# E imprimimos cada una de sus descripciones para mirar sus principales diferencias.

print(descripcion1)

print(descripcion2)

print(descripcion3)

print(descripcion4)

"""### Conclusiones de la Estadistica Descriptiva de los clusters generados.
<small>

- Cluster #1 = Nos  dimos cuenta Suelen ser los jugadores que se encuentran en etapas más avanzadas de sus carreras deportivas, con **edades típicamente entre los 27 y 38 años**. Tienen un alto potencial y rendimiento general, lo que sugiere que son **jugadores experimentados y altamente habilidosos**. Poseen una altura promedio y suelen destacar por su fortaleza física, lo que les permite competir de manera efectiva en el campo. Además, es probable que este grupo esté compuesto por veteranos y líderes en sus equipos, aportando experiencia y calidad a sus actuaciones.
- Cluster #2 = Por otro lado, en esta segunda clasificación suelen ser jugadores más jóvenes, en etapas tempranas de sus carreras deportivas, **con edades típicamente entre los 17 y 37 años**. Aunque tienen un potencial y rendimiento general más moderados en comparación con el Cluster 1, aún **muestran promesa y habilidades en desarrollo**. Además, tienen una altura promedio similar al Cluster 1, lo que sugiere que pueden estar en camino de desarrollar una presencia física significativa en el campo y es probable que este grupo esté compuesto por talentos emergentes y promesas jóvenes, quienes están en proceso de establecerse y destacarse en el mundo del deporte.
- Cluster #3 = En esta tercera clasificación nos encontramos con que suelen representar una mezcla entre jugadores jóvenes y experimentados, **con edades típicamente entre los 20 y 30 años**. Tienen un potencial y rendimiento general moderados, lo que sugiere que son jugadores sólidos pero no necesariamente estrellas destacadas. Destacan por tener la altura promedio más alta entre los clusters, lo que podría indicar que ocupan posiciones que requieren **mayor estatura**. Es probable además que este grupo esté compuesto por jugadores en una etapa estable de sus carreras, que contribuyen de manera consistente al equipo pero quizás no sean los protagonistas principales.
- Cluster #4 = Finalmente en esta última clasificación nos encontramos con que suelen representar una combinación entre jugadores jóvenes y experimentados, **con edades típicamente entre los 20 y 30 años**. Tienen un potencial y rendimiento general sólidos, lo que sugiere que son jugadores consistentes y confiables en el campo. Aunque tienen una **altura promedio más baja** en comparación con otros clusters, aún poseen las habilidades necesarias para competir efectivamente en su posición y es probable que este grupo esté compuesto por **jugadores versátiles y completos**, que pueden desempeñar múltiples roles en el equipo y contribuir de manera equilibrada en diferentes aspectos del juego.
</small>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Leer el dataset
df = pd.read_csv('fifa21_male2.csv', low_memory=False)
df = df.sample(n=1000, random_state=42)

# Preprocesamiento y limpieza de datos
df = df.drop(["Hits", "Value", "Wage", "Release Clause", "ID", "Name", "Club", "Position",
              "Player Photo", "Club Logo", "Flag Photo", "Team & Contract", "Growth", "Joined",
              "Loan Date End", "Contract", "Gender", "Composure", "W/F", "SM", "A/W", "D/W",
              "IR", "LS", "ST", "RS", "LW", "LF", "CF", "RF", "RW", "LAM", "CAM", "RAM", "LM",
              "LCM", "CM", "RCM", "RM", "LWB", "LDM", "CDM", "RDM", "RWB", "LB", "LCB", "CB",
              "RCB", "RB", "GK", "BP", "BOV", "foot"], axis=1)

def convertir_a_cm(altura):
    pies, pulgadas = altura.split("'")
    pies = int(pies)
    pulgadas = int(pulgadas[:-1])
    altura_cm = (pies * 12 + pulgadas) * 2.54
    return altura_cm

def convertir_a_kg(peso):
    peso_lbs = int(peso[:-3])
    peso_kg = peso_lbs * 0.453592
    return peso_kg

df['Weight'] = df['Weight'].apply(convertir_a_kg)
df['Height'] = df['Height'].apply(convertir_a_cm)

# 1. Acá realizamos la imputación:
# Eliminar filas con valores nulos
df = df.dropna()

# 2. Acá convertimos las categorías en números:
# Codificación de frecuencia para "Nationality"
frecuencia = df['Nationality'].value_counts()
df['nationality_frecuencia'] = df['Nationality'].map(frecuencia)

# Eliminar columna "Nationality"
df = df.drop(["Nationality"], axis=1)

# División del dataset
X = df.drop(columns=['OVA'])
y = df['OVA']

# Dividir los datos en conjuntos de entrenamiento, validación y prueba
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# 3. Acá normalizamos:
# Escalar las características
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# 5. Acá usamos un algoritmo para cumplir con el objetivo:
# Definir el modelo
model = RandomForestRegressor(random_state=42)

# 6. Acá cambiamos los hiperparámetros:
# Definir el rango de hiperparámetros a probar
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [10, 20, None]
}

# 8. Repetir 6 y 7 varias veces hasta lograr una buena medida. (Varios for para cambiar los hiperparámetros):
# Utilizar GridSearchCV para encontrar los mejores hiperparámetros
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(X_train, y_train)

# 7. Mostrar medidas de rendimiento (función de costo) tanto en train como en val:
# Mejor modelo encontrado
best_model = grid_search.best_estimator_

# Predicciones en el conjunto de entrenamiento
y_train_pred = best_model.predict(X_train)

# Predicciones en el conjunto de validación
y_val_pred = best_model.predict(X_val)

# Medidas de rendimiento
train_mse = mean_squared_error(y_train, y_train_pred)
val_mse = mean_squared_error(y_val, y_val_pred)

print(f'Train Mean Squared Error: {train_mse}')
print(f'Validation Mean Squared Error: {val_mse}')

# 9. Sacar el error de test:
# Predicciones en el conjunto de prueba
y_test_pred = best_model.predict(X_test)

# Medidas de rendimiento en el conjunto de prueba
test_mse = mean_squared_error(y_test, y_test_pred)

print(f'Test Mean Squared Error: {test_mse}')

# 10. Predecir un dato nuevo (inventado):
# Crear un nuevo dato inventado con las mismas características que el conjunto de entrenamiento
new_data = pd.DataFrame({
    'Age': [33],
    'nationality_frecuencia': [100],
    'POT': [93],
    'Height': [152],
    'Weight': [159],
    'Base Stats': [466],
    'PAC': [85],
    'SHO': [92],
    'PAS': [91],
    'DRI': [95],
    'DEF': [38],
    'PHY': [65],
    'Attacking': [0],
    'Crossing': [0],
    'Finishing': [0],
    'Heading Accuracy': [0],
    'Short Passing': [0],
    'Volleys': [0],
    'Skill': [0],
    'Dribbling': [0],
    'Curve': [0],
    'FK Accuracy': [0],
    'Long Passing': [0],
    'Ball Control': [0],
    'Movement': [0],
    'Acceleration': [0],
    'Sprint Speed': [0],
    'Agility': [0],
    'Reactions': [0],
    'Balance': [0],
    'Power': [0],
    'Shot Power': [0],
    'Jumping': [0],
    'Stamina': [0],
    'Strength': [0],
    'Long Shots': [0],
    'Mentality': [0],
    'Aggression': [0],
    'Interceptions': [0],
    'Positioning': [0],
    'Vision': [0],
    'Penalties': [0],
    'Defending': [0],
    'Marking': [0],
    'Standing Tackle': [0],
    'Sliding Tackle': [0],
    'Goalkeeping': [0],
    'GK Diving': [0],
    'GK Handling': [0],
    'GK Kicking': [0],
    'GK Positioning': [0],
    'GK Reflexes': [0],
    'Total Stats': [0]
})

# Asegurarse de que las columnas estén en el mismo orden que las características de entrenamiento
new_data = new_data[X.columns]

# Escalar el nuevo dato
new_data_scaled = scaler.transform(new_data)

# Predecir la etiqueta para el nuevo dato
new_prediction = best_model.predict(new_data_scaled)

# Asegurarse de que la predicción está en el rango válido (0-100):
new_prediction = np.clip(new_prediction, 0, 100)

print(f'Predicción para el nuevo dato: {new_prediction}')